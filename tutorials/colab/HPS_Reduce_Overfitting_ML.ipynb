{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b29a6a",
   "metadata": {
    "id": "84b29a6a"
   },
   "source": [
    "# Hyperparameter Search to reduce overfitting in Machine Learning (Scikit-Learn)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deephyper/tutorials/blob/main/tutorials/colab/HPS_Reduce_Overfitting_ML.ipynb)\n",
    "\n",
    "In this tutorial, we will show how to treat a learning method as a hyperparameter in the hyperparameter search. We will consider [Random Forest (RF)](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) classifier and [Gradient Boosting (GB)](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) classifier methods in [Scikit-Learn](https://scikit-learn.org/stable/) for the Airlines data set. Each of these methods have its own set of hyperparameters and some common parameters. We model them using ConfigSpace a python package to express conditional hyperparameters and more.\n",
    "\n",
    "Let us start by installing DeepHyper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7351e5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7351e5c",
    "outputId": "6de08914-122c-402e-da3a-8c43112fa253"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deephyper in /usr/local/lib/python3.7/dist-packages (0.3.3)\n",
      "Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from deephyper) (1.3.0)\n",
      "Requirement already satisfied: openml==0.10.2 in /usr/local/lib/python3.7/dist-packages (from deephyper) (0.10.2)\n",
      "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.7/dist-packages (from deephyper) (0.14.1)\n",
      "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from deephyper) (2.11.3)\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (from deephyper) (0.90)\n",
      "Requirement already satisfied: matplotlib>=3.0.3 in /usr/local/lib/python3.7/dist-packages (from deephyper) (3.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from deephyper) (1.0.1)\n",
      "Requirement already satisfied: dh-scikit-optimize==0.9.4 in /usr/local/lib/python3.7/dist-packages (from deephyper) (0.9.4)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from deephyper) (1.1.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from deephyper) (4.62.3)\n",
      "Requirement already satisfied: tensorflow>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from deephyper) (2.7.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deephyper) (1.19.5)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from deephyper) (2.6.3)\n",
      "Requirement already satisfied: ray[default]>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from deephyper) (1.8.0)\n",
      "Requirement already satisfied: ConfigSpace>=0.4.18 in /usr/local/lib/python3.7/dist-packages (from deephyper) (0.4.20)\n",
      "Requirement already satisfied: typeguard in /usr/local/lib/python3.7/dist-packages (from deephyper) (2.7.1)\n",
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from deephyper) (0.10.2)\n",
      "Requirement already satisfied: joblib>=0.10.3 in /usr/local/lib/python3.7/dist-packages (from deephyper) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from dh-scikit-optimize==0.9.4->deephyper) (1.4.1)\n",
      "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from dh-scikit-optimize==0.9.4->deephyper) (21.10.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from openml==0.10.2->deephyper) (2.23.0)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from openml==0.10.2->deephyper) (2.8.2)\n",
      "Requirement already satisfied: xmltodict in /usr/local/lib/python3.7/dist-packages (from openml==0.10.2->deephyper) (0.12.0)\n",
      "Requirement already satisfied: liac-arff>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from openml==0.10.2->deephyper) (2.5.0)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from ConfigSpace>=0.4.18->deephyper) (2.4.7)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from ConfigSpace>=0.4.18->deephyper) (0.29.24)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->deephyper) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->deephyper) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.2->deephyper) (2018.9)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->dh-scikit-optimize==0.9.4->deephyper) (3.13)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->openml==0.10.2->deephyper) (1.15.0)\n",
      "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray[default]>=1.3.0->deephyper) (1.41.1)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[default]>=1.3.0->deephyper) (7.1.2)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[default]>=1.3.0->deephyper) (2.6.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[default]>=1.3.0->deephyper) (1.0.2)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray[default]>=1.3.0->deephyper) (21.2.0)\n",
      "Requirement already satisfied: redis>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ray[default]>=1.3.0->deephyper) (4.0.1)\n",
      "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray[default]>=1.3.0->deephyper) (3.17.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[default]>=1.3.0->deephyper) (3.3.2)\n",
      "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from ray[default]>=1.3.0->deephyper) (0.12.0)\n",
      "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ray[default]>=1.3.0->deephyper) (0.3.11)\n",
      "Requirement already satisfied: aiohttp-cors in /usr/local/lib/python3.7/dist-packages (from ray[default]>=1.3.0->deephyper) (0.7.0)\n",
      "Requirement already satisfied: aioredis<2 in /usr/local/lib/python3.7/dist-packages (from ray[default]>=1.3.0->deephyper) (1.3.1)\n",
      "Requirement already satisfied: colorful in /usr/local/lib/python3.7/dist-packages (from ray[default]>=1.3.0->deephyper) (0.5.4)\n",
      "Requirement already satisfied: gpustat>=1.0.0b1 in /usr/local/lib/python3.7/dist-packages (from ray[default]>=1.3.0->deephyper) (1.0.0b1)\n",
      "Requirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.7/dist-packages (from ray[default]>=1.3.0->deephyper) (3.8.1)\n",
      "Requirement already satisfied: opencensus in /usr/local/lib/python3.7/dist-packages (from ray[default]>=1.3.0->deephyper) (0.8.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.7->ray[default]>=1.3.0->deephyper) (2.0.7)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.7->ray[default]>=1.3.0->deephyper) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.7->ray[default]>=1.3.0->deephyper) (3.10.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.7->ray[default]>=1.3.0->deephyper) (1.7.2)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.7->ray[default]>=1.3.0->deephyper) (0.13.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.7->ray[default]>=1.3.0->deephyper) (4.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.7->ray[default]>=1.3.0->deephyper) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.7->ray[default]>=1.3.0->deephyper) (5.2.0)\n",
      "Requirement already satisfied: hiredis in /usr/local/lib/python3.7/dist-packages (from aioredis<2->ray[default]>=1.3.0->deephyper) (2.0.0)\n",
      "Requirement already satisfied: blessed>=1.17.1 in /usr/local/lib/python3.7/dist-packages (from gpustat>=1.0.0b1->ray[default]>=1.3.0->deephyper) (1.19.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from gpustat>=1.0.0b1->ray[default]>=1.3.0->deephyper) (5.4.8)\n",
      "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from gpustat>=1.0.0b1->ray[default]>=1.3.0->deephyper) (7.352.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.7/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0b1->ray[default]>=1.3.0->deephyper) (0.2.5)\n",
      "Requirement already satisfied: deprecated in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray[default]>=1.3.0->deephyper) (1.2.13)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.1->deephyper) (3.0.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->deephyper) (0.12.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->deephyper) (3.1.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->deephyper) (0.22.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->deephyper) (2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->deephyper) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->deephyper) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->deephyper) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->deephyper) (2.7.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->deephyper) (12.0.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->deephyper) (1.1.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->deephyper) (0.37.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->deephyper) (1.13.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->deephyper) (1.1.2)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->deephyper) (0.4.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->deephyper) (2.7.0)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->deephyper) (2.7.0)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.0.0->deephyper) (1.5.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->deephyper) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->deephyper) (1.8.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->deephyper) (57.4.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->deephyper) (1.35.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->deephyper) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->deephyper) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->deephyper) (3.3.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0.0->deephyper) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0.0->deephyper) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0.0->deephyper) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.0.0->deephyper) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.0.0->deephyper) (4.8.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0.0->deephyper) (0.4.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->openml==0.10.2->deephyper) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->openml==0.10.2->deephyper) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->openml==0.10.2->deephyper) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->openml==0.10.2->deephyper) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.0.0->deephyper) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.0.0->deephyper) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->deephyper) (2.0.1)\n",
      "Requirement already satisfied: opencensus-context==0.1.2 in /usr/local/lib/python3.7/dist-packages (from opencensus->ray[default]>=1.3.0->deephyper) (0.1.2)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from opencensus->ray[default]>=1.3.0->deephyper) (1.26.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=1.3.0->deephyper) (21.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=1.3.0->deephyper) (1.53.0)\n",
      "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels->deephyper) (0.5.2)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability->deephyper) (0.1.6)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability->deephyper) (4.4.2)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability->deephyper) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install deephyper\n",
    "!pip install ray\n",
    "!pip install openml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce57d8f",
   "metadata": {},
   "source": [
    "We start by creating a function which loads the data of interest. Here we use the [\"Airlines\" dataset from OpenML](https://www.openml.org/search?type=data&sort=runs&id=1169&status=active) where the task is to predict whether a given flight will be delayed, given the information of the scheduled departure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b399d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import openml\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def load_data(\n",
    "    random_state=42,\n",
    "    verbose=False,\n",
    "    test_size=0.33,\n",
    "    valid_size=0.33,\n",
    "    categoricals_to_integers=False,\n",
    "):\n",
    "    \"\"\"Load the \"Airlines\" dataset from OpenML.\n",
    "\n",
    "    Args:\n",
    "        random_state (int, optional): A numpy `RandomState`. Defaults to 42.\n",
    "        verbose (bool, optional): Print informations about the dataset. Defaults to False.\n",
    "        test_size (float, optional): The proportion of the test dataset out of the whole data. Defaults to 0.33.\n",
    "        valid_size (float, optional): The proportion of the train dataset out of the whole data without the test data. Defaults to 0.33.\n",
    "        categoricals_to_integers (bool, optional): Convert categoricals features to integer values. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Numpy arrays as, `(X_train, y_train), (X_valid, y_valid), (X_test, y_test)`.\n",
    "    \"\"\"\n",
    "    random_state = (\n",
    "        np.random.RandomState(random_state) if type(random_state) is int else random_state\n",
    "    )\n",
    "\n",
    "    dataset = openml.datasets.get_dataset(1169)\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"This is dataset '{dataset.name}', the target feature is \"\n",
    "            f\"'{dataset.default_target_attribute}'\"\n",
    "        )\n",
    "        print(f\"URL: {dataset.url}\")\n",
    "        print(dataset.description[:500])\n",
    "\n",
    "    X, y, categorical_indicator, ft_names = dataset.get_data(\n",
    "        target=dataset.default_target_attribute\n",
    "    )\n",
    "\n",
    "    # encode categoricals as integers\n",
    "    if categoricals_to_integers:\n",
    "        for (ft_ind, ft_name) in enumerate(ft_names):\n",
    "            if categorical_indicator[ft_ind]:\n",
    "                labenc = LabelEncoder().fit(X[ft_name])\n",
    "                X[ft_name] = labenc.transform(X[ft_name])\n",
    "                n_classes = len(labenc.classes_)\n",
    "            else:\n",
    "                n_classes = -1\n",
    "            categorical_indicator[ft_ind] = (\n",
    "                categorical_indicator[ft_ind],\n",
    "                n_classes,\n",
    "            )\n",
    "\n",
    "    X, y = X.to_numpy(), y.to_numpy()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, shuffle=True, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # relative valid_size on Train set\n",
    "    r_valid_size = valid_size / (1.0 - test_size)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X_train, y_train, test_size=r_valid_size, shuffle=True, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return (X_train, y_train), (X_valid, y_valid), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce95907",
   "metadata": {
    "id": "7ce95907"
   },
   "source": [
    "Then, we create a mapping to record the classification algorithms of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b5ce3f",
   "metadata": {
    "id": "f7b5ce3f"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "\n",
    "CLASSIFIERS = {\n",
    "    \"RandomForest\": RandomForestClassifier,\n",
    "    \"GradientBoosting\": GradientBoostingClassifier,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b325396",
   "metadata": {
    "id": "4b325396"
   },
   "source": [
    "Create a baseline code to test the accuracy of the default configuration for both models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00027653",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00027653",
    "outputId": "9e555197-8d26-45c3-ef60-24cb2cea74a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest\n",
      "Accuracy on Training: 0.879\n",
      "Accuracy on Validation: 0.620\n",
      "Accuracy on Testing: 0.620\n",
      "\n",
      "GradientBoosting\n",
      "Accuracy on Training: 0.649\n",
      "Accuracy on Validation: 0.648\n",
      "Accuracy on Testing: 0.649\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import check_random_state\n",
    "\n",
    "rs_clf = check_random_state(42)\n",
    "rs_data = check_random_state(42)\n",
    "\n",
    "ratio_test = 0.33\n",
    "ratio_valid = (1 - ratio_test) * 0.33\n",
    "\n",
    "train, valid, test = load_data(\n",
    "    random_state=rs_data,\n",
    "    test_size=ratio_test,\n",
    "    valid_size=ratio_valid,\n",
    "    categoricals_to_integers=True,\n",
    ")\n",
    "\n",
    "for clf_name, clf_class in CLASSIFIERS.items():\n",
    "    print(clf_name)\n",
    "\n",
    "    clf = clf_class(random_state=rs_clf)\n",
    "\n",
    "    clf.fit(*train)\n",
    "\n",
    "    acc_train = clf.score(*train)\n",
    "    acc_valid = clf.score(*valid)\n",
    "    acc_test = clf.score(*test)\n",
    "\n",
    "    print(f\"Accuracy on Training: {acc_train:.3f}\")\n",
    "    print(f\"Accuracy on Validation: {acc_valid:.3f}\")\n",
    "    print(f\"Accuracy on Testing: {acc_test:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b501b3",
   "metadata": {
    "id": "64b501b3"
   },
   "source": [
    "The accuracy values show that the RandomForest classifier with default hyperparameters results in overfitting and thus poor generalization (high accuracy on training data but not on the validation and test data). On the contrary GradientBoosting does not show any sign of overfitting and has a better accuracy on the validation and testing set, which shows a better generalization than RandomForest.\n",
    "\n",
    "Next, we optimize the hyperparameters, where we seek to find the right classifier and its corresponding hyperparameters to improve the accuracy on the vaidation and test data. Create a `load_data` function to load and return training and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb93095f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bb93095f",
    "outputId": "b32c3934-992a-4eb2-b94c-01357cc0ced7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With subsampling\n",
      "X_train shape: (10000, 7)\n",
      "y_train shape: (10000,)\n",
      "X_valid shape: (119258, 7)\n",
      "y_valid shape: (119258,)\n",
      "\n",
      "Without subsampling\n",
      "X_train shape: (242128, 7)\n",
      "y_train shape: (242128,)\n",
      "X_valid shape: (119258, 7)\n",
      "y_valid shape: (119258,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def load_subsampled_data(verbose=0, subsample=True):\n",
    "\n",
    "    # In this case passing a random state is critical to make sure\n",
    "    # that the same data are loaded all the time and that the test set\n",
    "    # is not mixed with either the training or validation set.\n",
    "    # It is important to not avoid setting a global seed for safety reasons.\n",
    "    random_state = np.random.RandomState(seed=42)\n",
    "\n",
    "    # Proportion of the test set on the full dataset\n",
    "    ratio_test = 0.33\n",
    "\n",
    "    # Proportion of the valid set on \"dataset \\ test set\"\n",
    "    # here we want the test and validation set to have same number of elements\n",
    "    ratio_valid = (1 - ratio_test) * 0.33\n",
    "\n",
    "    # The 3rd result is ignored with \"_\" because it corresponds to the test set\n",
    "    # which is not interesting for us now.\n",
    "    (X_train, y_train), (X_valid, y_valid), _ = load_data(\n",
    "        random_state=random_state,\n",
    "        test_size=ratio_test,\n",
    "        valid_size=ratio_valid,\n",
    "        categoricals_to_integers=True,\n",
    "    )\n",
    "\n",
    "    # Uncomment the next line if you want to sub-sample the training data to speed-up\n",
    "    # the search, \"n_samples\" controls the size of the new training data\n",
    "    if subsample:\n",
    "        X_train, y_train = resample(X_train, y_train, n_samples=int(1e4))\n",
    "        \n",
    "    if verbose:\n",
    "        print(f\"X_train shape: {np.shape(X_train)}\")\n",
    "        print(f\"y_train shape: {np.shape(y_train)}\")\n",
    "        print(f\"X_valid shape: {np.shape(X_valid)}\")\n",
    "        print(f\"y_valid shape: {np.shape(y_valid)}\")\n",
    "    return (X_train, y_train), (X_valid, y_valid)\n",
    "\n",
    "print(\"With subsampling\")\n",
    "_ = load_subsampled_data(verbose=1)\n",
    "print(\"\\nWithout subsampling\")\n",
    "_ = load_subsampled_data(verbose=1, subsample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4ba367",
   "metadata": {
    "id": "8d4ba367"
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "<b>Tip</b> \n",
    "    \n",
    "Subsampling with `X_train, y_train = resample(X_train, y_train, n_samples=int(1e4))` can be useful if you want to speed-up your search. By subsampling the training time will reduce.\n",
    "    \n",
    "</div>\n",
    "\n",
    "Create a `run` function to train and evaluate a given hyperparameter configuration. This function has to return a scalar value (typically, validation accuracy), which will be maximized by the search algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c70fb1ee",
   "metadata": {
    "id": "c70fb1ee"
   },
   "outputs": [],
   "source": [
    "from inspect import signature\n",
    "\n",
    "def filter_parameters(obj, config: dict) -> dict:\n",
    "    \"\"\"Filter the incoming configuration dict based on the signature of obj.\n",
    "    Args:\n",
    "        obj (Callable): the object for which the signature is used.\n",
    "        config (dict): the configuration to filter.\n",
    "    Returns:\n",
    "        dict: the filtered configuration dict.\n",
    "    \"\"\"\n",
    "    sig = signature(obj)\n",
    "    clf_allowed_params = list(sig.parameters.keys())\n",
    "    clf_params = {\n",
    "        k: v\n",
    "        for k, v in config.items()\n",
    "        if k in clf_allowed_params and not (v in [\"nan\", \"NA\"])\n",
    "    }\n",
    "    return clf_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a2f3e17",
   "metadata": {
    "id": "5a2f3e17"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "\n",
    "def run(config: dict) -> float:\n",
    "\n",
    "    config[\"random_state\"] = check_random_state(42)\n",
    "\n",
    "    (X_train, y_train), (X_valid, y_valid) = load_subsampled_data()\n",
    "\n",
    "    clf_class = CLASSIFIERS[config[\"classifier\"]]\n",
    "\n",
    "    # keep parameters possible for the current classifier\n",
    "    config[\"n_jobs\"] = 4\n",
    "    clf_params = filter_parameters(clf_class, config)\n",
    "\n",
    "    try:  # good practice to manage the fail value yourself...\n",
    "        clf = clf_class(**clf_params)\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        fit_is_complete = True\n",
    "    except:\n",
    "        fit_is_complete = False\n",
    "\n",
    "    if fit_is_complete:\n",
    "        y_pred = clf.predict(X_valid)\n",
    "        acc = accuracy_score(y_valid, y_pred)\n",
    "    else:\n",
    "        acc = -1.0\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c11701",
   "metadata": {
    "id": "b7c11701"
   },
   "source": [
    "Create the `HpProblem` to define the search space of hyperparameters for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e39b561",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1e39b561",
    "outputId": "6035e693-fc90-4595-c656-b06db9dece26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Configuration space object:\n",
       "  Hyperparameters:\n",
       "    classifier, Type: Categorical, Choices: {RandomForest, GradientBoosting}, Default: RandomForest\n",
       "    criterion, Type: Categorical, Choices: {friedman_mse, squared_error, gini, entropy}, Default: gini\n",
       "    learning_rate, Type: UniformFloat, Range: [0.01, 1.0], Default: 0.505\n",
       "    loss, Type: Categorical, Choices: {deviance, exponential}, Default: deviance\n",
       "    max_depth, Type: UniformInteger, Range: [1, 50], Default: 26\n",
       "    min_samples_leaf, Type: UniformInteger, Range: [1, 10], Default: 6\n",
       "    min_samples_split, Type: UniformInteger, Range: [2, 10], Default: 6\n",
       "    n_estimators, Type: UniformInteger, Range: [1, 1000], Default: 32, on log-scale\n",
       "    subsample, Type: UniformFloat, Range: [0.01, 1.0], Default: 0.505\n",
       "  Conditions:\n",
       "    learning_rate | classifier == 'GradientBoosting'\n",
       "    loss | classifier == 'GradientBoosting'\n",
       "    subsample | classifier == 'GradientBoosting'\n",
       "  Forbidden Clauses:\n",
       "    (Forbidden: classifier == 'RandomForest' && Forbidden: criterion in {'friedman_mse', 'squared_error'})\n",
       "    (Forbidden: classifier == 'GradientBoosting' && Forbidden: criterion in {'entropy', 'gini'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ConfigSpace as cs\n",
    "from deephyper.problem import HpProblem\n",
    "\n",
    "\n",
    "problem = HpProblem()\n",
    "\n",
    "#! Default value are very important when adding conditional and forbidden clauses\n",
    "#! Otherwise the creation of the problem can fail if the default configuration is not\n",
    "#! Acceptable\n",
    "classifier = problem.add_hyperparameter(\n",
    "    [\"RandomForest\", \"GradientBoosting\"], \n",
    "    \"classifier\", \n",
    "    default_value=\"RandomForest\"\n",
    ")\n",
    "\n",
    "# For both\n",
    "problem.add_hyperparameter((1, 1000, \"log-uniform\"), \"n_estimators\")\n",
    "problem.add_hyperparameter((1, 50), \"max_depth\")\n",
    "problem.add_hyperparameter((2, 10), \"min_samples_split\")\n",
    "problem.add_hyperparameter((1, 10), \"min_samples_leaf\")\n",
    "criterion = problem.add_hyperparameter(\n",
    "    [\"friedman_mse\", \"squared_error\", \"gini\", \"entropy\"],\n",
    "    \"criterion\",\n",
    "    default_value=\"gini\",\n",
    ")\n",
    "\n",
    "# GradientBoosting\n",
    "loss = problem.add_hyperparameter([\"log_loss\", \"exponential\"], \"loss\")\n",
    "learning_rate = problem.add_hyperparameter((0.01, 1.0), \"learning_rate\")\n",
    "subsample = problem.add_hyperparameter((0.01, 1.0), \"subsample\")\n",
    "\n",
    "gradient_boosting_hp = [loss, learning_rate, subsample]\n",
    "for hp_i in gradient_boosting_hp:\n",
    "    problem.add_condition(cs.EqualsCondition(hp_i, classifier, \"GradientBoosting\"))\n",
    "\n",
    "forbidden_criterion_rf = cs.ForbiddenAndConjunction(\n",
    "    cs.ForbiddenEqualsClause(classifier, \"RandomForest\"),\n",
    "    cs.ForbiddenInClause(criterion, [\"friedman_mse\", \"squared_error\"]),\n",
    ")\n",
    "problem.add_forbidden_clause(forbidden_criterion_rf)\n",
    "\n",
    "forbidden_criterion_gb = cs.ForbiddenAndConjunction(\n",
    "    cs.ForbiddenEqualsClause(classifier, \"GradientBoosting\"),\n",
    "    cs.ForbiddenInClause(criterion, [\"gini\", \"entropy\"]),\n",
    ")\n",
    "problem.add_forbidden_clause(forbidden_criterion_gb)\n",
    "\n",
    "problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b452fe5b",
   "metadata": {
    "id": "b452fe5b"
   },
   "source": [
    "Create an `Evaluator` object using the `ray` backend to distribute the evaluation of the run-function defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "197f723d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "197f723d",
    "outputId": "ee12c0f0-3e76-4932-fbb4-c19329c278e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/romainegele/Documents/Argonne/deephyper/deephyper/evaluator/_evaluator.py:99: UserWarning: Applying nest-asyncio patch for IPython Shell!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of workers:  1\n"
     ]
    }
   ],
   "source": [
    "from deephyper.evaluator import Evaluator\n",
    "from deephyper.evaluator.callback import TqdmCallback\n",
    "\n",
    "evaluator = Evaluator.create(run, \n",
    "                 method=\"ray\", \n",
    "                 method_kwargs={\n",
    "                     \"address\": None, \n",
    "                     \"num_cpus\": 1,\n",
    "                     \"num_cpus_per_task\": 1,\n",
    "                     \"callbacks\": [TqdmCallback()]\n",
    "                     \n",
    "                 })\n",
    "\n",
    "print(\"Number of workers: \", evaluator.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6cd751",
   "metadata": {
    "id": "1b6cd751"
   },
   "source": [
    "Finally, you can define a Bayesian optimization search called `CBO` (for Centralized Bayesian Optimization) and link to it the defined `problem` and `evaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2e09062",
   "metadata": {
    "id": "b2e09062"
   },
   "outputs": [],
   "source": [
    "from deephyper.search.hps import CBO\n",
    "\n",
    "search = CBO(problem, evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0892e8a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0892e8a0",
    "outputId": "787ec3e1-d48d-4a14-d4f4-5a91d4d44eb5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:00<00:00, 8612.53it/s, objective=0.556]\u001b[2m\u001b[36m(run pid=2779)\u001b[0m /Users/romainegele/miniforge3/envs/dh-env-test/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "\u001b[2m\u001b[36m(run pid=2779)\u001b[0m   warnings.warn(\n",
      " 23%|██▎       | 7/30 [00:14<00:42,  1.85s/it, objective=0.639]  \u001b[2m\u001b[36m(run pid=2779)\u001b[0m /Users/romainegele/miniforge3/envs/dh-env-test/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "\u001b[2m\u001b[36m(run pid=2779)\u001b[0m   warnings.warn(\n",
      " 30%|███       | 9/30 [00:23<01:00,  2.90s/it, objective=0.639]\u001b[2m\u001b[36m(run pid=2779)\u001b[0m /Users/romainegele/miniforge3/envs/dh-env-test/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "\u001b[2m\u001b[36m(run pid=2779)\u001b[0m   warnings.warn(\n",
      " 70%|███████   | 21/30 [00:49<00:20,  2.23s/it, objective=0.643]\u001b[2m\u001b[36m(run pid=2779)\u001b[0m /Users/romainegele/miniforge3/envs/dh-env-test/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "\u001b[2m\u001b[36m(run pid=2779)\u001b[0m   warnings.warn(\n",
      "100%|██████████| 30/30 [01:35<00:00,  3.53s/it, objective=0.643]"
     ]
    }
   ],
   "source": [
    "results = search.search(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02b4096",
   "metadata": {
    "id": "c02b4096"
   },
   "source": [
    "Once the search is over, a file named `results.csv` is saved in the current directory. The same dataframe is returned by the `search.search(...)` call. It contains the hyperparameters configurations evaluated during the search and their corresponding `objective` value (i.e, validation accuracy), `timestamp_submit` the time when the evaluator submitted the configuration to be evaluated and `timestamp_gather` the time when the evaluator received the configuration once evaluated (both are relative times with respect to the creation of the `Evaluator` instance). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15e7aed4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 979
    },
    "id": "15e7aed4",
    "outputId": "12ad57ba-c62c-4343-c302-c5791c540827"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>criterion</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>subsample</th>\n",
       "      <th>job_id</th>\n",
       "      <th>objective</th>\n",
       "      <th>timestamp_submit</th>\n",
       "      <th>timestamp_gather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>48</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.034711</td>\n",
       "      <td>exponential</td>\n",
       "      <td>0.557088</td>\n",
       "      <td>1</td>\n",
       "      <td>0.556206</td>\n",
       "      <td>195.940520</td>\n",
       "      <td>196.982956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>83</td>\n",
       "      <td>0.648618</td>\n",
       "      <td>deviance</td>\n",
       "      <td>0.845242</td>\n",
       "      <td>2</td>\n",
       "      <td>0.530556</td>\n",
       "      <td>197.307517</td>\n",
       "      <td>202.533807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.639018</td>\n",
       "      <td>202.909171</td>\n",
       "      <td>203.645449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>entropy</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0.623195</td>\n",
       "      <td>203.949518</td>\n",
       "      <td>204.382671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>159</td>\n",
       "      <td>0.396828</td>\n",
       "      <td>exponential</td>\n",
       "      <td>0.185283</td>\n",
       "      <td>5</td>\n",
       "      <td>0.576867</td>\n",
       "      <td>204.742501</td>\n",
       "      <td>207.740379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>201</td>\n",
       "      <td>0.675658</td>\n",
       "      <td>exponential</td>\n",
       "      <td>0.093068</td>\n",
       "      <td>6</td>\n",
       "      <td>0.545054</td>\n",
       "      <td>208.046596</td>\n",
       "      <td>210.421025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>gini</td>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>0.590862</td>\n",
       "      <td>210.784200</td>\n",
       "      <td>211.211072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>120</td>\n",
       "      <td>0.035546</td>\n",
       "      <td>deviance</td>\n",
       "      <td>0.959819</td>\n",
       "      <td>8</td>\n",
       "      <td>0.612554</td>\n",
       "      <td>211.518923</td>\n",
       "      <td>219.358192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.738049</td>\n",
       "      <td>exponential</td>\n",
       "      <td>0.428511</td>\n",
       "      <td>9</td>\n",
       "      <td>0.605460</td>\n",
       "      <td>219.715949</td>\n",
       "      <td>220.191334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.716434</td>\n",
       "      <td>deviance</td>\n",
       "      <td>0.982813</td>\n",
       "      <td>10</td>\n",
       "      <td>0.589017</td>\n",
       "      <td>220.492764</td>\n",
       "      <td>221.167651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>entropy</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>0.630331</td>\n",
       "      <td>222.088602</td>\n",
       "      <td>222.842585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>entropy</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>0.637592</td>\n",
       "      <td>223.773805</td>\n",
       "      <td>224.587676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>entropy</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>171</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>0.641047</td>\n",
       "      <td>225.542043</td>\n",
       "      <td>226.755629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>gini</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>390</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "      <td>0.643085</td>\n",
       "      <td>227.746110</td>\n",
       "      <td>229.931809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>gini</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>0.635588</td>\n",
       "      <td>230.840223</td>\n",
       "      <td>231.389424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>gini</td>\n",
       "      <td>49</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>0.637173</td>\n",
       "      <td>232.353651</td>\n",
       "      <td>233.128984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>gini</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>756</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>0.637299</td>\n",
       "      <td>234.078103</td>\n",
       "      <td>238.228378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>entropy</td>\n",
       "      <td>34</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>247</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>0.639672</td>\n",
       "      <td>239.180137</td>\n",
       "      <td>240.811394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>entropy</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>161</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>0.638255</td>\n",
       "      <td>241.790899</td>\n",
       "      <td>243.062049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>entropy</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>0.627371</td>\n",
       "      <td>244.008439</td>\n",
       "      <td>244.938569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>gini</td>\n",
       "      <td>43</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>0.634842</td>\n",
       "      <td>245.905826</td>\n",
       "      <td>246.723595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>549</td>\n",
       "      <td>0.378506</td>\n",
       "      <td>deviance</td>\n",
       "      <td>0.929188</td>\n",
       "      <td>22</td>\n",
       "      <td>0.603934</td>\n",
       "      <td>247.684792</td>\n",
       "      <td>272.272688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>gini</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>185</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>0.635781</td>\n",
       "      <td>273.230332</td>\n",
       "      <td>274.586677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>gini</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "      <td>0.580548</td>\n",
       "      <td>275.530294</td>\n",
       "      <td>275.956395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>gini</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>209</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25</td>\n",
       "      <td>0.639856</td>\n",
       "      <td>276.930515</td>\n",
       "      <td>278.251332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>gini</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>226</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>0.639462</td>\n",
       "      <td>279.211143</td>\n",
       "      <td>280.672105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>gini</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27</td>\n",
       "      <td>0.626868</td>\n",
       "      <td>281.648273</td>\n",
       "      <td>282.120234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>gini</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>338</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "      <td>0.640787</td>\n",
       "      <td>283.128350</td>\n",
       "      <td>285.001436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>entropy</td>\n",
       "      <td>42</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>0.580624</td>\n",
       "      <td>286.002433</td>\n",
       "      <td>286.430476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>entropy</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>912</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "      <td>0.636393</td>\n",
       "      <td>287.421778</td>\n",
       "      <td>292.209157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          classifier      criterion  max_depth  min_samples_leaf  \\\n",
       "0   GradientBoosting   friedman_mse         48                 8   \n",
       "1   GradientBoosting  squared_error         25                 6   \n",
       "2       RandomForest        entropy         10                 2   \n",
       "3       RandomForest        entropy          3                 5   \n",
       "4   GradientBoosting   friedman_mse         40                 8   \n",
       "5   GradientBoosting  squared_error         16                 9   \n",
       "6       RandomForest           gini         31                 7   \n",
       "7   GradientBoosting  squared_error         27                 7   \n",
       "8   GradientBoosting   friedman_mse          7                 7   \n",
       "9   GradientBoosting  squared_error         23                10   \n",
       "10      RandomForest        entropy         21                 2   \n",
       "11      RandomForest        entropy         13                 9   \n",
       "12      RandomForest        entropy         14                 9   \n",
       "13      RandomForest           gini         14                 9   \n",
       "14      RandomForest           gini         14                 9   \n",
       "15      RandomForest           gini         49                 9   \n",
       "16      RandomForest           gini         25                 6   \n",
       "17      RandomForest        entropy         34                 9   \n",
       "18      RandomForest        entropy         35                 6   \n",
       "19      RandomForest        entropy         33                 2   \n",
       "20      RandomForest           gini         43                 8   \n",
       "21  GradientBoosting  squared_error         49                 1   \n",
       "22      RandomForest           gini         48                 6   \n",
       "23      RandomForest           gini         17                10   \n",
       "24      RandomForest           gini         12                10   \n",
       "25      RandomForest           gini         17                10   \n",
       "26      RandomForest           gini         17                 9   \n",
       "27      RandomForest           gini         12                 8   \n",
       "28      RandomForest        entropy         42                10   \n",
       "29      RandomForest        entropy         40                10   \n",
       "\n",
       "    min_samples_split  n_estimators  learning_rate         loss  subsample  \\\n",
       "0                   6             1       0.034711  exponential   0.557088   \n",
       "1                   7            83       0.648618     deviance   0.845242   \n",
       "2                   2            80            NaN          NaN        NaN   \n",
       "3                   5             3            NaN          NaN        NaN   \n",
       "4                   7           159       0.396828  exponential   0.185283   \n",
       "5                   8           201       0.675658  exponential   0.093068   \n",
       "6                   3             2            NaN          NaN        NaN   \n",
       "7                   7           120       0.035546     deviance   0.959819   \n",
       "8                   4             5       0.738049  exponential   0.428511   \n",
       "9                   5             5       0.716434     deviance   0.982813   \n",
       "10                  9            57            NaN          NaN        NaN   \n",
       "11                  7            85            NaN          NaN        NaN   \n",
       "12                  7           171            NaN          NaN        NaN   \n",
       "13                  9           390            NaN          NaN        NaN   \n",
       "14                 10            28            NaN          NaN        NaN   \n",
       "15                  6            76            NaN          NaN        NaN   \n",
       "16                  9           756            NaN          NaN        NaN   \n",
       "17                  9           247            NaN          NaN        NaN   \n",
       "18                  8           161            NaN          NaN        NaN   \n",
       "19                  2            82            NaN          NaN        NaN   \n",
       "20                  7            81            NaN          NaN        NaN   \n",
       "21                  9           549       0.378506     deviance   0.929188   \n",
       "22                  9           185            NaN          NaN        NaN   \n",
       "23                  9             1            NaN          NaN        NaN   \n",
       "24                  9           209            NaN          NaN        NaN   \n",
       "25                  9           226            NaN          NaN        NaN   \n",
       "26                  9             9            NaN          NaN        NaN   \n",
       "27                  3           338            NaN          NaN        NaN   \n",
       "28                  6             1            NaN          NaN        NaN   \n",
       "29                  7           912            NaN          NaN        NaN   \n",
       "\n",
       "    job_id  objective  timestamp_submit  timestamp_gather  \n",
       "0        1   0.556206        195.940520        196.982956  \n",
       "1        2   0.530556        197.307517        202.533807  \n",
       "2        3   0.639018        202.909171        203.645449  \n",
       "3        4   0.623195        203.949518        204.382671  \n",
       "4        5   0.576867        204.742501        207.740379  \n",
       "5        6   0.545054        208.046596        210.421025  \n",
       "6        7   0.590862        210.784200        211.211072  \n",
       "7        8   0.612554        211.518923        219.358192  \n",
       "8        9   0.605460        219.715949        220.191334  \n",
       "9       10   0.589017        220.492764        221.167651  \n",
       "10      11   0.630331        222.088602        222.842585  \n",
       "11      12   0.637592        223.773805        224.587676  \n",
       "12      13   0.641047        225.542043        226.755629  \n",
       "13      14   0.643085        227.746110        229.931809  \n",
       "14      15   0.635588        230.840223        231.389424  \n",
       "15      16   0.637173        232.353651        233.128984  \n",
       "16      17   0.637299        234.078103        238.228378  \n",
       "17      18   0.639672        239.180137        240.811394  \n",
       "18      19   0.638255        241.790899        243.062049  \n",
       "19      20   0.627371        244.008439        244.938569  \n",
       "20      21   0.634842        245.905826        246.723595  \n",
       "21      22   0.603934        247.684792        272.272688  \n",
       "22      23   0.635781        273.230332        274.586677  \n",
       "23      24   0.580548        275.530294        275.956395  \n",
       "24      25   0.639856        276.930515        278.251332  \n",
       "25      26   0.639462        279.211143        280.672105  \n",
       "26      27   0.626868        281.648273        282.120234  \n",
       "27      28   0.640787        283.128350        285.001436  \n",
       "28      29   0.580624        286.002433        286.430476  \n",
       "29      30   0.636393        287.421778        292.209157  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d51e46",
   "metadata": {
    "id": "c6d51e46"
   },
   "source": [
    "The `deephyper-analytics` command line is a way of analyzing this type of file. For example, we want to output the best configuration we can use the `topk` functionnality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7da7c6b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7da7c6b",
    "outputId": "38fab841-3c7f-472b-87e2-b390607374f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'0':\n",
      "  classifier: RandomForest\n",
      "  criterion: gini\n",
      "  job_id: 14\n",
      "  learning_rate: null\n",
      "  loss: null\n",
      "  max_depth: 14\n",
      "  min_samples_leaf: 9\n",
      "  min_samples_split: 9\n",
      "  n_estimators: 390\n",
      "  objective: 0.6430847406\n",
      "  subsample: null\n",
      "  timestamp_gather: 229.9318091869\n",
      "  timestamp_submit: 227.7461099625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!deephyper-analytics topk -k 1 results.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba059c1",
   "metadata": {
    "id": "bba059c1"
   },
   "source": [
    "Let us define a test to evaluate the best configuration on the training, validation and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "081b52ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "081b52ec",
    "outputId": "526a4038-9816-4a03-eaa4-e702918e6cc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best config is:\n",
      "{'classifier': 'RandomForest',\n",
      " 'criterion': 'gini',\n",
      " 'job_id': 14,\n",
      " 'learning_rate': nan,\n",
      " 'loss': nan,\n",
      " 'max_depth': 14,\n",
      " 'min_samples_leaf': 9,\n",
      " 'min_samples_split': 9,\n",
      " 'n_estimators': 390,\n",
      " 'objective': 0.6430847406463298,\n",
      " 'subsample': nan}\n",
      "Accuracy on Training: 0.702\n",
      "Accuracy on Validation: 0.663\n",
      "Accuracy on Testing: 0.664\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "config = results.iloc[results.objective.argmax()][:-2].to_dict()\n",
    "print(\"Best config is:\")\n",
    "pprint(config)\n",
    "\n",
    "config[\"random_state\"] = check_random_state(42)\n",
    "\n",
    "rs_data = check_random_state(42)\n",
    "\n",
    "ratio_test = 0.33\n",
    "ratio_valid = (1 - ratio_test) * 0.33\n",
    "\n",
    "train, valid, test = load_data(\n",
    "    random_state=rs_data,\n",
    "    test_size=ratio_test,\n",
    "    valid_size=ratio_valid,\n",
    "    categoricals_to_integers=True,\n",
    ")\n",
    "\n",
    "clf_class = CLASSIFIERS[config[\"classifier\"]]\n",
    "config[\"n_jobs\"] = 4\n",
    "clf_params = filter_parameters(clf_class, config)\n",
    "\n",
    "clf = clf_class(**clf_params)\n",
    "\n",
    "clf.fit(*train)\n",
    "\n",
    "acc_train = clf.score(*train)\n",
    "acc_valid = clf.score(*valid)\n",
    "acc_test = clf.score(*test)\n",
    "\n",
    "print(f\"Accuracy on Training: {acc_train:.3f}\")\n",
    "print(f\"Accuracy on Validation: {acc_valid:.3f}\")\n",
    "print(f\"Accuracy on Testing: {acc_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0307f865",
   "metadata": {
    "id": "0307f865"
   },
   "source": [
    "Compared to the default configuration, we can see the accuracy improvement and the reduction of overfitting between the training and  the validation/test data sets."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "HPS_Reduce_Overfitting_ML.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "9dc039a2a4f4ad8a9dc018393b0776cc00e1bb4d428a37e9ad776085656c6f7f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dh-env-test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
