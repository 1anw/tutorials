{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b29a6a",
   "metadata": {},
   "source": [
    "# Hyperparameter Search to reduce overfitting in Machine Learning (Scikit-Learn)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deephyper/tutorials/blob/main/tutorials/colab/HPS_Reduce_Overfitting_ML.ipynb)\n",
    "\n",
    "In this tutorial, we will show how to treat a learning method as a hyperparameter in the hyperparameter search. We will consider [Random Forest (RF)](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) classifier and [Gradient Boosting (GB)](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) classifier methods in [Scikit-Learn](https://scikit-learn.org/stable/) for the Airlines data set. Each of these methods have its own set of hyperparameters and some common parameters. We model them using ConfigSpace a python package to express conditional hyperparameters and more.\n",
    "\n",
    "Let us start by installing DeepHyper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7351e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deephyper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7f9c98",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Warning</b>\n",
    "    \n",
    "By design asyncio does not allow nested event loops. Jupyter is using Tornado which already starts an event loop. Therefore the following patch is required to run this tutorial.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a0e70dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in /Users/romainegele/opt/anaconda3/envs/dh-dev/lib/python3.8/site-packages (1.5.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nest_asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce95907",
   "metadata": {},
   "source": [
    "Create a mapping to record the classification algorithms of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7b5ce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "\n",
    "CLASSIFIERS = {\n",
    "    \"RandomForest\": RandomForestClassifier,\n",
    "    \"GradientBoosting\": GradientBoostingClassifier,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b325396",
   "metadata": {},
   "source": [
    "Create a baseline code to test the accuracy of the default configuration for both models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00027653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest\n",
      "Accuracy on Training: 0.879\n",
      "Accuracy on Validation: 0.620\n",
      "Accuracy on Testing: 0.620\n",
      "\n",
      "GradientBoosting\n",
      "Accuracy on Training: 0.649\n",
      "Accuracy on Validation: 0.648\n",
      "Accuracy on Testing: 0.649\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from deephyper.benchmark.datasets import airlines as dataset\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "rs_clf = check_random_state(42)\n",
    "rs_data = check_random_state(42)\n",
    "\n",
    "ratio_test = 0.33\n",
    "ratio_valid = (1 - ratio_test) * 0.33\n",
    "\n",
    "train, valid, test, _ = dataset.load_data(\n",
    "    random_state=rs_data,\n",
    "    test_size=ratio_test,\n",
    "    valid_size=ratio_valid,\n",
    "    categoricals_to_integers=True,\n",
    ")\n",
    "\n",
    "for clf_name, clf_class in CLASSIFIERS.items():\n",
    "    print(clf_name)\n",
    "\n",
    "    clf = clf_class(random_state=rs_clf)\n",
    "\n",
    "    clf.fit(*train)\n",
    "\n",
    "    acc_train = clf.score(*train)\n",
    "    acc_valid = clf.score(*valid)\n",
    "    acc_test = clf.score(*test)\n",
    "\n",
    "    print(f\"Accuracy on Training: {acc_train:.3f}\")\n",
    "    print(f\"Accuracy on Validation: {acc_valid:.3f}\")\n",
    "    print(f\"Accuracy on Testing: {acc_test:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b501b3",
   "metadata": {},
   "source": [
    "The accuracy values show that the RandomForest classifier with default hyperparameters results in overfitting and thus poor generalization (high accuracy on training data but not on the validation and test data). On the contrary GradientBoosting does not show any sign of overfitting and has a better accuracy on the validation and testing set, which shows a better generalization than RandomForest.\n",
    "\n",
    "Next, we optimize the hyperparameters, where we seek to find the right classifier and its corresponding hyperparameters to improve the accuracy on the vaidation and test data. Create a `load_data` function to load and return training and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb93095f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With subsampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (10000, 7)\n",
      "y_train shape: (10000,)\n",
      "X_valid shape: (119258, 7)\n",
      "y_valid shape: (119258,)\n",
      "\n",
      "Without subsampling\n",
      "X_train shape: (242128, 7)\n",
      "y_train shape: (242128,)\n",
      "X_valid shape: (119258, 7)\n",
      "y_valid shape: (119258,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def load_data(verbose=0, subsample=True):\n",
    "\n",
    "    # In this case passing a random state is critical to make sure\n",
    "    # that the same data are loaded all the time and that the test set\n",
    "    # is not mixed with either the training or validation set.\n",
    "    # It is important to not avoid setting a global seed for safety reasons.\n",
    "    random_state = np.random.RandomState(seed=42)\n",
    "\n",
    "    # Proportion of the test set on the full dataset\n",
    "    ratio_test = 0.33\n",
    "\n",
    "    # Proportion of the valid set on \"dataset \\ test set\"\n",
    "    # here we want the test and validation set to have same number of elements\n",
    "    ratio_valid = (1 - ratio_test) * 0.33\n",
    "\n",
    "    # The 3rd result is ignored with \"_\" because it corresponds to the test set\n",
    "    # which is not interesting for us now.\n",
    "    (X_train, y_train), (X_valid, y_valid), _, _ = dataset.load_data(\n",
    "        random_state=random_state,\n",
    "        test_size=ratio_test,\n",
    "        valid_size=ratio_valid,\n",
    "        categoricals_to_integers=True,\n",
    "    )\n",
    "\n",
    "    # Uncomment the next line if you want to sub-sample the training data to speed-up\n",
    "    # the search, \"n_samples\" controls the size of the new training data\n",
    "    if subsample:\n",
    "        X_train, y_train = resample(X_train, y_train, n_samples=int(1e4))\n",
    "        \n",
    "    if verbose:\n",
    "        print(f\"X_train shape: {np.shape(X_train)}\")\n",
    "        print(f\"y_train shape: {np.shape(y_train)}\")\n",
    "        print(f\"X_valid shape: {np.shape(X_valid)}\")\n",
    "        print(f\"y_valid shape: {np.shape(y_valid)}\")\n",
    "    return (X_train, y_train), (X_valid, y_valid)\n",
    "\n",
    "print(\"With subsampling\")\n",
    "_ = load_data(verbose=1)\n",
    "print(\"\\nWithout subsampling\")\n",
    "_ = load_data(verbose=1, subsample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4ba367",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "<b>Tip</b> \n",
    "    \n",
    "Subsampling with `X_train, y_train = resample(X_train, y_train, n_samples=int(1e4))` can be useful if you want to speed-up your search. By subsampling the training time will reduce.\n",
    "    \n",
    "</div>\n",
    "\n",
    "Create a `run` function to train and evaluate a given hyperparameter configuration. This function has to return a scalar value (typically, validation accuracy), which will be maximized by the search algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c70fb1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import signature\n",
    "\n",
    "def filter_parameters(obj, config: dict) -> dict:\n",
    "    \"\"\"Filter the incoming configuration dict based on the signature of obj.\n",
    "    Args:\n",
    "        obj (Callable): the object for which the signature is used.\n",
    "        config (dict): the configuration to filter.\n",
    "    Returns:\n",
    "        dict: the filtered configuration dict.\n",
    "    \"\"\"\n",
    "    sig = signature(obj)\n",
    "    clf_allowed_params = list(sig.parameters.keys())\n",
    "    clf_params = {\n",
    "        k: v\n",
    "        for k, v in config.items()\n",
    "        if k in clf_allowed_params and not (v in [\"nan\", \"NA\"])\n",
    "    }\n",
    "    return clf_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a2f3e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "\n",
    "def run(config: dict) -> float:\n",
    "\n",
    "    config[\"random_state\"] = check_random_state(42)\n",
    "\n",
    "    (X_train, y_train), (X_valid, y_valid) = load_data()\n",
    "\n",
    "    clf_class = CLASSIFIERS[config[\"classifier\"]]\n",
    "\n",
    "    # keep parameters possible for the current classifier\n",
    "    config[\"n_jobs\"] = 4\n",
    "    clf_params = filter_parameters(clf_class, config)\n",
    "\n",
    "    try:  # good practice to manage the fail value yourself...\n",
    "        clf = clf_class(**clf_params)\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        fit_is_complete = True\n",
    "    except:\n",
    "        fit_is_complete = False\n",
    "\n",
    "    if fit_is_complete:\n",
    "        y_pred = clf.predict(X_valid)\n",
    "        acc = accuracy_score(y_valid, y_pred)\n",
    "    else:\n",
    "        acc = -1.0\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c11701",
   "metadata": {},
   "source": [
    "Create the `HpProblem` to define the search space of hyperparameters for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e39b561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration space object:\n",
      "  Hyperparameters:\n",
      "    classifier, Type: Categorical, Choices: {RandomForest, GradientBoosting}, Default: RandomForest\n",
      "    criterion, Type: Categorical, Choices: {friedman_mse, squared_error, gini, entropy}, Default: gini\n",
      "    learning_rate, Type: UniformFloat, Range: [0.01, 1.0], Default: 0.505\n",
      "    loss, Type: Categorical, Choices: {deviance, exponential}, Default: deviance\n",
      "    max_depth, Type: UniformInteger, Range: [1, 50], Default: 26\n",
      "    min_samples_leaf, Type: UniformInteger, Range: [1, 10], Default: 6\n",
      "    min_samples_split, Type: UniformInteger, Range: [2, 10], Default: 6\n",
      "    n_estimators, Type: UniformInteger, Range: [1, 1000], Default: 32, on log-scale\n",
      "    subsample, Type: UniformFloat, Range: [0.01, 1.0], Default: 0.505\n",
      "  Conditions:\n",
      "    learning_rate | classifier == 'GradientBoosting'\n",
      "    loss | classifier == 'GradientBoosting'\n",
      "    subsample | classifier == 'GradientBoosting'\n",
      "  Forbidden Clauses:\n",
      "    (Forbidden: classifier == 'RandomForest' && Forbidden: criterion in {'friedman_mse', 'squared_error'})\n",
      "    (Forbidden: classifier == 'GradientBoosting' && Forbidden: criterion in {'entropy', 'gini'})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ConfigSpace as cs\n",
    "from deephyper.problem import HpProblem\n",
    "\n",
    "\n",
    "problem = HpProblem()\n",
    "\n",
    "#! Default value are very important when adding conditional and forbidden clauses\n",
    "#! Otherwise the creation of the problem can fail if the default configuration is not\n",
    "#! Acceptable\n",
    "classifier = problem.add_hyperparameter(\n",
    "    name=\"classifier\",\n",
    "    value=[\"RandomForest\", \"GradientBoosting\"],\n",
    "    default_value=\"RandomForest\",\n",
    ")\n",
    "\n",
    "# For both\n",
    "problem.add_hyperparameter(name=\"n_estimators\", value=(1, 1000, \"log-uniform\"))\n",
    "problem.add_hyperparameter(name=\"max_depth\", value=(1, 50))\n",
    "problem.add_hyperparameter(\n",
    "    name=\"min_samples_split\", value=(2, 10),\n",
    ")\n",
    "problem.add_hyperparameter(name=\"min_samples_leaf\", value=(1, 10))\n",
    "criterion = problem.add_hyperparameter(\n",
    "    name=\"criterion\",\n",
    "    value=[\"friedman_mse\", \"squared_error\", \"gini\", \"entropy\"],\n",
    "    default_value=\"gini\",\n",
    ")\n",
    "\n",
    "# GradientBoosting\n",
    "loss = problem.add_hyperparameter(name=\"loss\", value=[\"deviance\", \"exponential\"])\n",
    "learning_rate = problem.add_hyperparameter(name=\"learning_rate\", value=(0.01, 1.0))\n",
    "subsample = problem.add_hyperparameter(name=\"subsample\", value=(0.01, 1.0))\n",
    "\n",
    "gradient_boosting_hp = [loss, learning_rate, subsample]\n",
    "for hp_i in gradient_boosting_hp:\n",
    "    problem.add_condition(cs.EqualsCondition(hp_i, classifier, \"GradientBoosting\"))\n",
    "\n",
    "forbidden_criterion_rf = cs.ForbiddenAndConjunction(\n",
    "    cs.ForbiddenEqualsClause(classifier, \"RandomForest\"),\n",
    "    cs.ForbiddenInClause(criterion, [\"friedman_mse\", \"squared_error\"]),\n",
    ")\n",
    "problem.add_forbidden_clause(forbidden_criterion_rf)\n",
    "\n",
    "forbidden_criterion_gb = cs.ForbiddenAndConjunction(\n",
    "    cs.ForbiddenEqualsClause(classifier, \"GradientBoosting\"),\n",
    "    cs.ForbiddenInClause(criterion, [\"gini\", \"entropy\"]),\n",
    ")\n",
    "problem.add_forbidden_clause(forbidden_criterion_gb)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b452fe5b",
   "metadata": {},
   "source": [
    "Create an `Evaluator` object using the `ray` backend to distribute the evaluation of the run-function defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "197f723d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of workers:  2\n"
     ]
    }
   ],
   "source": [
    "from deephyper.evaluator import Evaluator\n",
    "from deephyper.evaluator.callback import LoggerCallback\n",
    "\n",
    "evaluator = Evaluator.create(run, \n",
    "                 method=\"ray\", \n",
    "                 method_kwargs={\n",
    "                     \"address\": None, \n",
    "                     \"num_cpus\": 2,\n",
    "                     \"num_cpus_per_task\": 1,\n",
    "                     \"callbacks\": [LoggerCallback()]\n",
    "                     \n",
    "                 })\n",
    "\n",
    "print(\"Number of workers: \", evaluator.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6cd751",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "<b>Tip</b> \n",
    "    \n",
    "You can open the ray-dashboard at an address like <a>http://127.0.0.1:port</a> in a browser to monitor the CPU usage of the execution.\n",
    "    \n",
    "</div>\n",
    "\n",
    "Finally, you can define a Bayesian optimization search called `AMBS` (for Asynchronous Model-Based Search) and link to it the defined `problem` and `evaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2e09062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deephyper.search.hps import AMBS\n",
    "\n",
    "search = AMBS(problem, evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0892e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00001] -- best objective: 0.61031 -- received objective: 0.61031\n",
      "[00002] -- best objective: 0.63312 -- received objective: 0.63312\n",
      "[00003] -- best objective: 0.63312 -- received objective: 0.61690\n",
      "[00004] -- best objective: 0.63602 -- received objective: 0.63602\n",
      "[00005] -- best objective: 0.63602 -- received objective: 0.62558\n",
      "[00006] -- best objective: 0.63602 -- received objective: 0.61756\n",
      "[00007] -- best objective: 0.63602 -- received objective: 0.56976\n",
      "[00008] -- best objective: 0.63790 -- received objective: 0.63790\n",
      "[00009] -- best objective: 0.63828 -- received objective: 0.63828\n",
      "[00010] -- best objective: 0.63889 -- received objective: 0.63889\n",
      "[00011] -- best objective: 0.63889 -- received objective: 0.55800\n",
      "[00012] -- best objective: 0.63893 -- received objective: 0.63893\n",
      "[00013] -- best objective: 0.63893 -- received objective: 0.59362\n",
      "[00014] -- best objective: 0.63934 -- received objective: 0.63934\n",
      "[00015] -- best objective: 0.63934 -- received objective: 0.61325\n",
      "[00016] -- best objective: 0.63934 -- received objective: 0.63594\n",
      "[00017] -- best objective: 0.63934 -- received objective: 0.60019\n",
      "[00018] -- best objective: 0.63934 -- received objective: 0.63890\n",
      "[00019] -- best objective: 0.63934 -- received objective: 0.59527\n",
      "[00020] -- best objective: 0.64127 -- received objective: 0.64127\n",
      "[00021] -- best objective: 0.64127 -- received objective: 0.61190\n",
      "[00022] -- best objective: 0.64127 -- received objective: 0.63981\n",
      "[00023] -- best objective: 0.64127 -- received objective: 0.52452\n",
      "[00024] -- best objective: 0.64127 -- received objective: 0.57789\n",
      "[00025] -- best objective: 0.64127 -- received objective: 0.61380\n",
      "[00026] -- best objective: 0.64140 -- received objective: 0.64140\n",
      "[00027] -- best objective: 0.64447 -- received objective: 0.64447\n",
      "[00028] -- best objective: 0.64447 -- received objective: 0.63760\n",
      "[00029] -- best objective: 0.64447 -- received objective: 0.61842\n",
      "[00030] -- best objective: 0.64447 -- received objective: 0.64000\n"
     ]
    }
   ],
   "source": [
    "results = search.search(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02b4096",
   "metadata": {},
   "source": [
    "Once the search is over, a file named `results.csv` is saved in the current directory. The same dataframe is returned by the `search.search(...)` call. It contains the hyperparameters configurations evaluated during the search and their corresponding `objective` value (i.e, validation accuracy), `duration` of computation and time of computation with `elapsed_sec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15e7aed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>criterion</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>subsample</th>\n",
       "      <th>id</th>\n",
       "      <th>objective</th>\n",
       "      <th>elapsed_sec</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>43</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.226690</td>\n",
       "      <td>exponential</td>\n",
       "      <td>0.508968</td>\n",
       "      <td>1</td>\n",
       "      <td>0.610307</td>\n",
       "      <td>7.266928</td>\n",
       "      <td>0.895765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>gini</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0.633123</td>\n",
       "      <td>7.425238</td>\n",
       "      <td>1.054060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.592971</td>\n",
       "      <td>exponential</td>\n",
       "      <td>0.997199</td>\n",
       "      <td>3</td>\n",
       "      <td>0.616898</td>\n",
       "      <td>8.246314</td>\n",
       "      <td>0.821558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>gini</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>0.636016</td>\n",
       "      <td>9.668707</td>\n",
       "      <td>1.099710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>entropy</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0.625576</td>\n",
       "      <td>11.967111</td>\n",
       "      <td>4.157838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>gini</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>0.617560</td>\n",
       "      <td>13.041344</td>\n",
       "      <td>0.777343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>0.413725</td>\n",
       "      <td>exponential</td>\n",
       "      <td>0.030497</td>\n",
       "      <td>8</td>\n",
       "      <td>0.569756</td>\n",
       "      <td>14.451965</td>\n",
       "      <td>1.108964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>entropy</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>0.637903</td>\n",
       "      <td>15.788199</td>\n",
       "      <td>5.836488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>gini</td>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>0.638280</td>\n",
       "      <td>17.183025</td>\n",
       "      <td>1.133997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>entropy</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>0.638892</td>\n",
       "      <td>18.380346</td>\n",
       "      <td>0.937433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>57</td>\n",
       "      <td>0.863116</td>\n",
       "      <td>deviance</td>\n",
       "      <td>0.900712</td>\n",
       "      <td>9</td>\n",
       "      <td>0.558000</td>\n",
       "      <td>19.471669</td>\n",
       "      <td>4.702864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>entropy</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>0.638934</td>\n",
       "      <td>19.909548</td>\n",
       "      <td>1.266897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>817</td>\n",
       "      <td>0.169935</td>\n",
       "      <td>deviance</td>\n",
       "      <td>0.128928</td>\n",
       "      <td>13</td>\n",
       "      <td>0.593621</td>\n",
       "      <td>25.005183</td>\n",
       "      <td>5.229698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>gini</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>0.639345</td>\n",
       "      <td>27.229065</td>\n",
       "      <td>1.965783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.082906</td>\n",
       "      <td>deviance</td>\n",
       "      <td>0.578675</td>\n",
       "      <td>16</td>\n",
       "      <td>0.613250</td>\n",
       "      <td>28.326463</td>\n",
       "      <td>0.838137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>entropy</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>0.635941</td>\n",
       "      <td>29.477788</td>\n",
       "      <td>0.886151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>407</td>\n",
       "      <td>0.084910</td>\n",
       "      <td>exponential</td>\n",
       "      <td>0.073046</td>\n",
       "      <td>18</td>\n",
       "      <td>0.600195</td>\n",
       "      <td>34.799271</td>\n",
       "      <td>5.052738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>gini</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>226</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>0.638901</td>\n",
       "      <td>37.184077</td>\n",
       "      <td>2.120262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>0.542585</td>\n",
       "      <td>exponential</td>\n",
       "      <td>0.816895</td>\n",
       "      <td>20</td>\n",
       "      <td>0.595272</td>\n",
       "      <td>39.857766</td>\n",
       "      <td>2.412598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>392</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>0.641274</td>\n",
       "      <td>42.856760</td>\n",
       "      <td>2.737279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>entropy</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>0.611900</td>\n",
       "      <td>43.888935</td>\n",
       "      <td>0.759301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>entropy</td>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>225</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>0.639815</td>\n",
       "      <td>46.404094</td>\n",
       "      <td>2.245448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>727</td>\n",
       "      <td>0.273069</td>\n",
       "      <td>deviance</td>\n",
       "      <td>0.409875</td>\n",
       "      <td>14</td>\n",
       "      <td>0.524518</td>\n",
       "      <td>48.224100</td>\n",
       "      <td>27.920621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.316333</td>\n",
       "      <td>exponential</td>\n",
       "      <td>0.448612</td>\n",
       "      <td>25</td>\n",
       "      <td>0.577890</td>\n",
       "      <td>49.279222</td>\n",
       "      <td>0.784698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>72</td>\n",
       "      <td>0.035578</td>\n",
       "      <td>deviance</td>\n",
       "      <td>0.994611</td>\n",
       "      <td>24</td>\n",
       "      <td>0.613804</td>\n",
       "      <td>53.795924</td>\n",
       "      <td>7.034713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>entropy</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>743</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>0.641399</td>\n",
       "      <td>55.589962</td>\n",
       "      <td>6.044412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>entropy</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>883</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27</td>\n",
       "      <td>0.644468</td>\n",
       "      <td>61.462345</td>\n",
       "      <td>7.305073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>gini</td>\n",
       "      <td>33</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "      <td>0.637601</td>\n",
       "      <td>63.571769</td>\n",
       "      <td>7.663768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>201</td>\n",
       "      <td>0.029695</td>\n",
       "      <td>exponential</td>\n",
       "      <td>0.748982</td>\n",
       "      <td>30</td>\n",
       "      <td>0.618424</td>\n",
       "      <td>76.395800</td>\n",
       "      <td>12.530569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>gini</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "      <td>0.639999</td>\n",
       "      <td>78.074152</td>\n",
       "      <td>1.364345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          classifier      criterion  max_depth  min_samples_leaf  \\\n",
       "0   GradientBoosting  squared_error         43                 9   \n",
       "1       RandomForest           gini         38                 7   \n",
       "2   GradientBoosting   friedman_mse         10                 4   \n",
       "3       RandomForest           gini         25                 8   \n",
       "4       RandomForest        entropy         38                 1   \n",
       "5       RandomForest           gini          6                 7   \n",
       "6   GradientBoosting   friedman_mse         23                10   \n",
       "7       RandomForest        entropy         23                 8   \n",
       "8       RandomForest           gini         34                10   \n",
       "9       RandomForest        entropy         32                10   \n",
       "10  GradientBoosting   friedman_mse         19                 4   \n",
       "11      RandomForest        entropy         24                10   \n",
       "12  GradientBoosting  squared_error          4                 8   \n",
       "13      RandomForest           gini         17                10   \n",
       "14  GradientBoosting   friedman_mse         22                10   \n",
       "15      RandomForest        entropy         17                10   \n",
       "16  GradientBoosting  squared_error         36                 7   \n",
       "17      RandomForest           gini         17                 9   \n",
       "18  GradientBoosting   friedman_mse         23                 4   \n",
       "19      RandomForest           gini         10                10   \n",
       "20      RandomForest        entropy         11                 9   \n",
       "21      RandomForest        entropy         29                10   \n",
       "22  GradientBoosting   friedman_mse         25                10   \n",
       "23  GradientBoosting   friedman_mse         28                 4   \n",
       "24  GradientBoosting  squared_error         30                 7   \n",
       "25      RandomForest        entropy         25                10   \n",
       "26      RandomForest        entropy         12                 9   \n",
       "27      RandomForest           gini         33                 9   \n",
       "28  GradientBoosting   friedman_mse         26                10   \n",
       "29      RandomForest           gini         17                 8   \n",
       "\n",
       "    min_samples_split  n_estimators  learning_rate         loss  subsample  \\\n",
       "0                   2             3       0.226690  exponential   0.508968   \n",
       "1                   8            18            NaN          NaN        NaN   \n",
       "2                   9             1       0.592971  exponential   0.997199   \n",
       "3                   4            53            NaN          NaN        NaN   \n",
       "4                   8           398            NaN          NaN        NaN   \n",
       "5                   2             2            NaN          NaN        NaN   \n",
       "6                   8            40       0.413725  exponential   0.030497   \n",
       "7                   5           713            NaN          NaN        NaN   \n",
       "8                   7            74            NaN          NaN        NaN   \n",
       "9                   6            34            NaN          NaN        NaN   \n",
       "10                  9            57       0.863116     deviance   0.900712   \n",
       "11                  7            87            NaN          NaN        NaN   \n",
       "12                  7           817       0.169935     deviance   0.128928   \n",
       "13                 10           200            NaN          NaN        NaN   \n",
       "14                  9             3       0.082906     deviance   0.578675   \n",
       "15                  6            25            NaN          NaN        NaN   \n",
       "16                  7           407       0.084910  exponential   0.073046   \n",
       "17                  6           226            NaN          NaN        NaN   \n",
       "18                 10            21       0.542585  exponential   0.816895   \n",
       "19                  2           392            NaN          NaN        NaN   \n",
       "20                  2             2            NaN          NaN        NaN   \n",
       "21                  2           225            NaN          NaN        NaN   \n",
       "22                  8           727       0.273069     deviance   0.409875   \n",
       "23                  3             1       0.316333  exponential   0.448612   \n",
       "24                 10            72       0.035578     deviance   0.994611   \n",
       "25                  3           743            NaN          NaN        NaN   \n",
       "26                  3           883            NaN          NaN        NaN   \n",
       "27                  9           906            NaN          NaN        NaN   \n",
       "28                  6           201       0.029695  exponential   0.748982   \n",
       "29                  2            76            NaN          NaN        NaN   \n",
       "\n",
       "    id  objective  elapsed_sec   duration  \n",
       "0    1   0.610307     7.266928   0.895765  \n",
       "1    2   0.633123     7.425238   1.054060  \n",
       "2    3   0.616898     8.246314   0.821558  \n",
       "3    5   0.636016     9.668707   1.099710  \n",
       "4    4   0.625576    11.967111   4.157838  \n",
       "5    7   0.617560    13.041344   0.777343  \n",
       "6    8   0.569756    14.451965   1.108964  \n",
       "7    6   0.637903    15.788199   5.836488  \n",
       "8   10   0.638280    17.183025   1.133997  \n",
       "9   11   0.638892    18.380346   0.937433  \n",
       "10   9   0.558000    19.471669   4.702864  \n",
       "11  12   0.638934    19.909548   1.266897  \n",
       "12  13   0.593621    25.005183   5.229698  \n",
       "13  15   0.639345    27.229065   1.965783  \n",
       "14  16   0.613250    28.326463   0.838137  \n",
       "15  17   0.635941    29.477788   0.886151  \n",
       "16  18   0.600195    34.799271   5.052738  \n",
       "17  19   0.638901    37.184077   2.120262  \n",
       "18  20   0.595272    39.857766   2.412598  \n",
       "19  21   0.641274    42.856760   2.737279  \n",
       "20  22   0.611900    43.888935   0.759301  \n",
       "21  23   0.639815    46.404094   2.245448  \n",
       "22  14   0.524518    48.224100  27.920621  \n",
       "23  25   0.577890    49.279222   0.784698  \n",
       "24  24   0.613804    53.795924   7.034713  \n",
       "25  26   0.641399    55.589962   6.044412  \n",
       "26  27   0.644468    61.462345   7.305073  \n",
       "27  28   0.637601    63.571769   7.663768  \n",
       "28  30   0.618424    76.395800  12.530569  \n",
       "29  31   0.639999    78.074152   1.364345  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d51e46",
   "metadata": {},
   "source": [
    "The `deephyper-analytics` command line is a way of analyzing this type of file. For example, we want to output the best configuration we can use the `topk` functionnality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7da7c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'0':\r\n",
      "  classifier: RandomForest\r\n",
      "  criterion: entropy\r\n",
      "  duration: 7.3050730228\r\n",
      "  elapsed_sec: 61.4623451233\r\n",
      "  id: 27\r\n",
      "  learning_rate: null\r\n",
      "  loss: null\r\n",
      "  max_depth: 12\r\n",
      "  min_samples_leaf: 9\r\n",
      "  min_samples_split: 3\r\n",
      "  n_estimators: 883\r\n",
      "  objective: 0.6444682956\r\n",
      "  subsample: null\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!deephyper-analytics topk results.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba059c1",
   "metadata": {},
   "source": [
    "Let us define a test to evaluate the best configuration on the training, validation and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "081b52ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best config is:\n",
      "{'classifier': 'RandomForest',\n",
      " 'criterion': 'entropy',\n",
      " 'id': 27,\n",
      " 'learning_rate': nan,\n",
      " 'loss': nan,\n",
      " 'max_depth': 12,\n",
      " 'min_samples_leaf': 9,\n",
      " 'min_samples_split': 3,\n",
      " 'n_estimators': 883,\n",
      " 'objective': 0.6444682956279663,\n",
      " 'subsample': nan}\n",
      "Accuracy on Training: 0.679\n",
      "Accuracy on Validation: 0.658\n",
      "Accuracy on Testing: 0.658\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "config = results.iloc[results.objective.argmax()][:-2].to_dict()\n",
    "print(\"Best config is:\")\n",
    "pprint(config)\n",
    "\n",
    "config[\"random_state\"] = check_random_state(42)\n",
    "\n",
    "rs_data = check_random_state(42)\n",
    "\n",
    "ratio_test = 0.33\n",
    "ratio_valid = (1 - ratio_test) * 0.33\n",
    "\n",
    "train, valid, test, _ = dataset.load_data(\n",
    "    random_state=rs_data,\n",
    "    test_size=ratio_test,\n",
    "    valid_size=ratio_valid,\n",
    "    categoricals_to_integers=True,\n",
    ")\n",
    "\n",
    "clf_class = CLASSIFIERS[config[\"classifier\"]]\n",
    "config[\"n_jobs\"] = 4\n",
    "clf_params = filter_parameters(clf_class, config)\n",
    "\n",
    "clf = clf_class(**clf_params)\n",
    "\n",
    "clf.fit(*train)\n",
    "\n",
    "acc_train = clf.score(*train)\n",
    "acc_valid = clf.score(*valid)\n",
    "acc_test = clf.score(*test)\n",
    "\n",
    "print(f\"Accuracy on Training: {acc_train:.3f}\")\n",
    "print(f\"Accuracy on Validation: {acc_valid:.3f}\")\n",
    "print(f\"Accuracy on Testing: {acc_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0307f865",
   "metadata": {},
   "source": [
    "Compared to the default configuration, we can see the accuracy improvement and the reduction of overfitting between the training and  the validation/test data sets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
