.. _tutorial-05:

Automated Machine Learning with Scikit-Learn
********************************************

.. warning::

    Be sure to work in a virtual environment where you can easily ``pip install`` new packages. This typically entails using either Anaconda, virtualenv, or Pipenv.

In this tutorial, we will show how to automatically search among different machine learning algorithms from `Scikit-Learn <https://scikit-learn.org/stable/>`_. Automated machine learning only requires the user to link the data with a predifined problem and run function that we provide.

Let us start by creating a DeepHyper project and a problem for our application:

.. code-block:: console
    :caption: bash

    deephyper start-project dh_project
    cd dh_project/dh_project/

Classification
==============

Let us create a problem for classification (from ``dh_project/dh_project/``):

.. code-block:: console
    :caption: bash

    deephyper new-problem hps autoclassifier
    cd autoclassifier/


Then, create ``model_run.py`` file to train and evaluate the model corresponding to the configuration generated by the search. This function has to return a scalar value (typically, validation accuracy), which will be maximized by the search algorithm. In the case of automated machine learning we use the run function provided at ``deephyper.sklearn.classifier.run`` and wrap it with our data such as:

.. literalinclude:: dh_project/dh_project/autoclassifier/model_run.py
    :caption: autoclassifier/model_run.py

We are ready to go! But, let us look at the problem provided by DeepHyper to understand better what is happening under the hood. The run function is the following. Just execute the problem module:

.. code-block:: bash

    python -m deephyper.sklearn.classifier.autosklearn1.problem

.. code-block:: python

    Configuration space object:
        Hyperparameters:
            C, Type: UniformFloat, Range: [1e-05, 10.0], Default: 0.01, on log-scale
            alpha, Type: UniformFloat, Range: [1e-05, 10.0], Default: 0.01, on log-scale
            classifier, Type: Categorical, Choices: {RandomForest, Logistic, AdaBoost, KNeighbors, MLP, SVC, XGBoost}, Default: RandomForest
            gamma, Type: UniformFloat, Range: [1e-05, 10.0], Default: 0.01, on log-scale
            kernel, Type: Categorical, Choices: {linear, poly, rbf, sigmoid}, Default: linear
            max_depth, Type: UniformInteger, Range: [2, 100], Default: 14, on log-scale
            n_estimators, Type: UniformInteger, Range: [1, 2000], Default: 45, on log-scale
            n_neighbors, Type: UniformInteger, Range: [1, 100], Default: 50
        Conditions:
            (C | classifier == 'Logistic' || C | classifier == 'SVC')
            (gamma | kernel == 'rbf' || gamma | kernel == 'poly' || gamma | kernel == 'sigmoid')
            (n_estimators | classifier == 'RandomForest' || n_estimators | classifier == 'AdaBoost')
            alpha | classifier == 'MLP'
            kernel | classifier == 'SVC'
            max_depth | classifier == 'RandomForest'
            n_neighbors | classifier == 'KNeighbors'

From the previous output you can see the different classifiers and hyperparameters tested in this search. Now you can run the search:

.. code-block:: console
    :caption: bash

    deephyper hps ambs --problem deephyper.sklearn.classifier.Problem --run dh_project.autoclassifier.model_run.run --max-evals 10 --num-workers 2

The ``results.csv`` file was created, to quickly visualize the best configuration run:

.. code-block:: console
    :caption: bash

    deephyper-analytics topk results.csv

.. code-block:: yaml

    '0':
        C: 0.0620705437
        alpha: null
        classifier: Logistic
        elapsed_sec: 12.8076210022
        gamma: null
        id: 80119c76-e961-11eb-ae48-acde48001122
        kernel: null
        max_depth: null
        n_estimators: null
        n_neighbors: null
        objective: 0.9840425532
        seed: 4154940283

Regression
==========

Let us create a problem for classification (from ``dh_project/dh_project/``):

.. code-block:: console
    :caption: bash

    deephyper new-problem hps autoregressor
    cd autoregressor/


Then, create ``model_run.py`` file to train and evaluate the model corresponding to the configuration generated by the search. This function has to return a scalar value (typically, negative mse or :math:`R^2`), which will be maximized by the search algorithm. In the case of automated machine learning we use the run function provided at ``deephyper.sklearn.regressor.run`` and wrap it with our data such as:

.. literalinclude:: dh_project/dh_project/autoregressor/model_run.py
    :caption: autoregressor/model_run.py

We are ready to go! But, let us look at the problem provided by DeepHyper to understand better what is happening under the hood. The run function is the following. Just execute the problem module:

.. code-block:: bash

    python -m deephyper.sklearn.regressor.autosklearn1.problem

.. code-block:: python

    Configuration space object:
        Hyperparameters:
            C, Type: UniformFloat, Range: [1e-05, 10.0], Default: 0.01, on log-scale
            alpha, Type: UniformFloat, Range: [1e-05, 10.0], Default: 0.01, on log-scale
            gamma, Type: UniformFloat, Range: [1e-05, 10.0], Default: 0.01, on log-scale
            kernel, Type: Categorical, Choices: {linear, poly, rbf, sigmoid}, Default: linear
            max_depth, Type: UniformInteger, Range: [2, 100], Default: 14, on log-scale
            n_estimators, Type: UniformInteger, Range: [1, 2000], Default: 45, on log-scale
            n_neighbors, Type: UniformInteger, Range: [1, 100], Default: 50
            regressor, Type: Categorical, Choices: {RandomForest, Linear, AdaBoost, KNeighbors, MLP, SVR, XGBoost}, Default: RandomForest
        Conditions:
            (gamma | kernel == 'rbf' || gamma | kernel == 'poly' || gamma | kernel == 'sigmoid')
            (n_estimators | regressor == 'RandomForest' || n_estimators | regressor == 'AdaBoost')
            C | regressor == 'SVR'
            alpha | regressor == 'MLP'
            kernel | regressor == 'SVR'
            max_depth | regressor == 'RandomForest'
            n_neighbors | regressor == 'KNeighbors'

From the previous output you can see the different classifiers and hyperparameters tested in this search. Now you can run the search:

.. code-block:: console
    :caption: bash

    deephyper hps ambs --problem deephyper.sklearn.regressor.Problem --run dh_project.autoregressor.model_run.run --max-evals 10 --num-workers 2

The ``results.csv`` file was created, to quickly visualize the best configuration run:

.. code-block:: console
    :caption: bash

    deephyper-analytics topk results.csv

.. code-block:: yaml

    '0':
        C: null
        alpha: null
        elapsed_sec: 13.2827298641
        gamma: null
        id: 69179500-f68d-11eb-9007-acde48001122
        kernel: null
        max_depth: 4.0
        n_estimators: 90.0
        n_neighbors: null
        objective: 0.8314073885
        regressor: RandomForest
        seed: 1603314780