{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc07e2ec",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Warning</b>\n",
    "    \n",
    "By design asyncio does not allow nested event loops. Jupyter is using Tornado which already starts an event loop. Therefore the following patch is required to run this tutorial.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a4b2f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in /Users/romainegele/opt/anaconda3/envs/dhtfp/lib/python3.8/site-packages (1.5.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nest_asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b29a6a",
   "metadata": {},
   "source": [
    "# Hyperparameter Search for Machine Learning (Basic)\n",
    "\n",
    "In this tutorial, we will show how to tune the hyperparameters of the [Random Forest (RF) classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html>)\n",
    "in [scikit-learn](https://scikit-learn.org/stable/) for the Airlines data set.\n",
    "\n",
    "\n",
    "Let us start by creating a function `run_baseline` to test the accuracy of the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b5ce3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training: 0.879\n",
      "Accuracy on Validation: 0.620\n",
      "Accuracy on Testing: 0.620\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from deephyper.benchmark.datasets import airlines as dataset\n",
    "    \n",
    "def run_baseline():\n",
    "\n",
    "    rs_data = np.random.RandomState(seed=42)\n",
    "\n",
    "    ratio_test = 0.33\n",
    "    ratio_valid = (1 - ratio_test) * 0.33\n",
    "\n",
    "    train, valid, test, _ = dataset.load_data(\n",
    "        random_state=rs_data,\n",
    "        test_size=ratio_test,\n",
    "        valid_size=ratio_valid,\n",
    "        categoricals_to_integers=True,\n",
    "    )\n",
    "\n",
    "    rs_classifier = check_random_state(42)\n",
    "\n",
    "    classifier = RandomForestClassifier(n_jobs=4, random_state=rs_classifier)\n",
    "    classifier.fit(*train)\n",
    "\n",
    "    acc_train = classifier.score(*train)\n",
    "    acc_valid = classifier.score(*valid)\n",
    "    acc_test = classifier.score(*test)\n",
    "\n",
    "    print(f\"Accuracy on Training: {acc_train:.3f}\")\n",
    "    print(f\"Accuracy on Validation: {acc_valid:.3f}\")\n",
    "    print(f\"Accuracy on Testing: {acc_test:.3f}\")\n",
    "\n",
    "\n",
    "run_baseline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b501b3",
   "metadata": {},
   "source": [
    "The accuracy values show that the RandomForest classifier with default hyperparameters results in overfitting and thus poor generalization (high accuracy on training data but not on the validation and test data).\n",
    "\n",
    "Next, we optimize the hyperparameters of the RandomForest classifier to address the overfitting problem and improve the accuracy on the validation and test data. Create the `load_data` function to load and return training and validation data.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "<b>Tip</b>\n",
    "    \n",
    "Subsampling with <code>X_train, y_train = resample(X_train, y_train, n_samples=int(1e4))</code>can be useful if you want to speed-up your search. By subsampling the training time will reduce.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb93095f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (242128, 7)\n",
      "y_train shape: (242128,)\n",
      "X_valid shape: (119258, 7)\n",
      "y_valid shape: (119258,)\n"
     ]
    }
   ],
   "source": [
    "def load_data(verbose=0):\n",
    "\n",
    "    # In this case passing a random state is critical to make sure\n",
    "    # that the same data are loaded all the time and that the test set\n",
    "    # is not mixed with either the training or validation set.\n",
    "    # It is important to not avoid setting a global seed for safety reasons.\n",
    "    random_state = np.random.RandomState(seed=42)\n",
    "\n",
    "    # Proportion of the test set on the full dataset\n",
    "    ratio_test = 0.33\n",
    "\n",
    "    # Proportion of the valid set on \"dataset \\ test set\"\n",
    "    # here we want the test and validation set to have same number of elements\n",
    "    ratio_valid = (1 - ratio_test) * 0.33\n",
    "\n",
    "    # The 3rd result is ignored with \"_\" because it corresponds to the test set\n",
    "    # which is not interesting for us now.\n",
    "    (X_train, y_train), (X_valid, y_valid), _, _ = dataset.load_data(\n",
    "        random_state=random_state,\n",
    "        test_size=ratio_test,\n",
    "        valid_size=ratio_valid,\n",
    "        categoricals_to_integers=True,\n",
    "    )\n",
    "\n",
    "    # Uncomment the next line if you want to sub-sample the training data to speed-up\n",
    "    # the search, \"n_samples\" controls the size of the new training data\n",
    "    # X_train, y_train = resample(X_train, y_train, n_samples=int(1e4))\n",
    "    if verbose:\n",
    "        print(f\"X_train shape: {np.shape(X_train)}\")\n",
    "        print(f\"y_train shape: {np.shape(y_train)}\")\n",
    "        print(f\"X_valid shape: {np.shape(X_valid)}\")\n",
    "        print(f\"y_valid shape: {np.shape(y_valid)}\")\n",
    "    return (X_train, y_train), (X_valid, y_valid)\n",
    "\n",
    "_  = load_data(verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4ba367",
   "metadata": {},
   "source": [
    "Create a function `run` to train and evaluate the RF model given a specific configuration `config`. This function has to return a scalar value (typically, validation accuracy), which will be maximized by the search algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca32c16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config: dict):\n",
    "\n",
    "    rs = check_random_state(42)\n",
    "\n",
    "    (X, y), (vX, vy) = load_data()\n",
    "\n",
    "    classifier = RandomForestClassifier(\n",
    "        n_jobs=4,\n",
    "        random_state=rs,\n",
    "        n_estimators=config[\"n_estimators\"],\n",
    "        criterion=config[\"criterion\"],\n",
    "        max_depth=config[\"max_depth\"],\n",
    "        min_samples_split=config[\"min_samples_split\"]\n",
    "    )\n",
    "    classifier.fit(X, y)\n",
    "\n",
    "    mean_accuracy = classifier.score(vX, vy)\n",
    "\n",
    "    return mean_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b85be31",
   "metadata": {},
   "source": [
    "Create a `HpProblem` instance named `problem` to define the search space of hyper-parameters for the RF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ce8cf19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Configuration space object:\n",
       "  Hyperparameters:\n",
       "    criterion, Type: Categorical, Choices: {gini, entropy}, Default: gini\n",
       "    max_depth, Type: UniformInteger, Range: [1, 50], Default: 26\n",
       "    min_samples_split, Type: UniformInteger, Range: [2, 10], Default: 6\n",
       "    n_estimators, Type: UniformInteger, Range: [10, 300], Default: 155\n",
       "\n",
       "\n",
       "  Starting Point:\n",
       "{0: {'criterion': 'gini',\n",
       "     'max_depth': 50,\n",
       "     'min_samples_split': 2,\n",
       "     'n_estimators': 100}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deephyper.problem import HpProblem\n",
    "\n",
    "problem = HpProblem()\n",
    "\n",
    "problem.add_hyperparameter((10, 300), \"n_estimators\")\n",
    "problem.add_hyperparameter([\"gini\", \"entropy\"], \"criterion\")\n",
    "problem.add_hyperparameter((1, 50), \"max_depth\")\n",
    "problem.add_hyperparameter((2, 10), \"min_samples_split\")\n",
    "\n",
    "# We define a starting point with the defaul hyperparameters from sklearn-learn\n",
    "# that we consider good in average.\n",
    "problem.add_starting_point(\n",
    "    n_estimators=100, criterion=\"gini\", max_depth=50, min_samples_split=2\n",
    ")\n",
    "\n",
    "problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b452fe5b",
   "metadata": {},
   "source": [
    "Create an `Evaluator` object using the `ray` backend to distribute the evaluation of the run-function defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "197f723d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 16:33:42,099\tINFO services.py:1267 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of workers:  2\n"
     ]
    }
   ],
   "source": [
    "from deephyper.evaluator.evaluate import Evaluator\n",
    "\n",
    "evaluator = Evaluator.create(run, \n",
    "                 method=\"ray\", \n",
    "                 method_kwargs={\n",
    "                     \"address\": None, \n",
    "                     \"num_cpus\": 2,\n",
    "                     \"num_cpus_per_task\": 1\n",
    "                 })\n",
    "\n",
    "print(\"Number of workers: \", evaluator.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6cd751",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "<b>Tip</b> \n",
    "    \n",
    "You can open the ray-dashboard at an address like <a>http://127.0.0.1:8266</a> in a browser to monitor the CPU usage of the execution.\n",
    "    \n",
    "</div>\n",
    "\n",
    "Finally, you can define a Bayesian optimization search called `AMBS` (for Asynchronous Model-Based Search) and link to it the defined `problem` and `evaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2e09062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deephyper.search.hps import AMBS\n",
    "\n",
    "search = AMBS(problem, evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0892e8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search.search(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02b4096",
   "metadata": {},
   "source": [
    "Once the search is over, a file named `results.csv` is saved in the current directory. The same dataframe is returned by the `search.search(...)` call. It contains the hyperparameters configurations evaluated during the search and their corresponding `objective` value (i.e, validation accuracy), `duration` of computation and time of computation with `elapsed_sec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15e7aed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>criterion</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>id</th>\n",
       "      <th>objective</th>\n",
       "      <th>elapsed_sec</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gini</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>0.635966</td>\n",
       "      <td>15.584538</td>\n",
       "      <td>12.659195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gini</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>0.620369</td>\n",
       "      <td>21.130914</td>\n",
       "      <td>18.205591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gini</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>4</td>\n",
       "      <td>0.630356</td>\n",
       "      <td>35.085798</td>\n",
       "      <td>13.778017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>entropy</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>125</td>\n",
       "      <td>3</td>\n",
       "      <td>0.620965</td>\n",
       "      <td>39.129643</td>\n",
       "      <td>23.444741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gini</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>0.649013</td>\n",
       "      <td>39.753345</td>\n",
       "      <td>4.484855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gini</td>\n",
       "      <td>49</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.621434</td>\n",
       "      <td>42.905477</td>\n",
       "      <td>2.999732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>entropy</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0.653985</td>\n",
       "      <td>45.822875</td>\n",
       "      <td>2.722389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gini</td>\n",
       "      <td>45</td>\n",
       "      <td>10</td>\n",
       "      <td>56</td>\n",
       "      <td>6</td>\n",
       "      <td>0.650204</td>\n",
       "      <td>47.139352</td>\n",
       "      <td>7.735802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>entropy</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>0.655243</td>\n",
       "      <td>49.793846</td>\n",
       "      <td>3.775522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>entropy</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "      <td>0.632100</td>\n",
       "      <td>54.876923</td>\n",
       "      <td>4.903988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>entropy</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>10</td>\n",
       "      <td>0.631438</td>\n",
       "      <td>55.052439</td>\n",
       "      <td>7.721802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>entropy</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0.651872</td>\n",
       "      <td>57.610671</td>\n",
       "      <td>2.583051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>entropy</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>0.658027</td>\n",
       "      <td>60.414571</td>\n",
       "      <td>2.616628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gini</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>103</td>\n",
       "      <td>15</td>\n",
       "      <td>0.663830</td>\n",
       "      <td>74.481396</td>\n",
       "      <td>13.887527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>entropy</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>13</td>\n",
       "      <td>0.653348</td>\n",
       "      <td>86.281291</td>\n",
       "      <td>31.083730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>entropy</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>288</td>\n",
       "      <td>16</td>\n",
       "      <td>0.663117</td>\n",
       "      <td>116.864489</td>\n",
       "      <td>42.191943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gini</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>286</td>\n",
       "      <td>17</td>\n",
       "      <td>0.652526</td>\n",
       "      <td>138.583843</td>\n",
       "      <td>52.065018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gini</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>250</td>\n",
       "      <td>18</td>\n",
       "      <td>0.657516</td>\n",
       "      <td>157.324089</td>\n",
       "      <td>40.178739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gini</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>300</td>\n",
       "      <td>19</td>\n",
       "      <td>0.657465</td>\n",
       "      <td>186.280438</td>\n",
       "      <td>47.488718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>298</td>\n",
       "      <td>20</td>\n",
       "      <td>0.652996</td>\n",
       "      <td>188.729754</td>\n",
       "      <td>31.203226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   criterion  max_depth  min_samples_split  n_estimators  id  objective  \\\n",
       "0       gini         35                  5            68   2   0.635966   \n",
       "1       gini         50                  2           100   1   0.620369   \n",
       "2       gini         38                  4            80   4   0.630356   \n",
       "3    entropy         33                  2           125   3   0.620965   \n",
       "4       gini          9                  2            37   5   0.649013   \n",
       "5       gini         49                  3            10   7   0.621434   \n",
       "6    entropy         17                  2            10   8   0.653985   \n",
       "7       gini         45                 10            56   6   0.650204   \n",
       "8    entropy         18                  2            21   9   0.655243   \n",
       "9    entropy         23                  2            22  11   0.632100   \n",
       "10   entropy         25                  3            44  10   0.631438   \n",
       "11   entropy         18                  2            11  12   0.651872   \n",
       "12   entropy         16                  7            10  14   0.658027   \n",
       "13      gini         15                  2           103  15   0.663830   \n",
       "14   entropy         19                  2           212  13   0.653348   \n",
       "15   entropy         14                  2           288  16   0.663117   \n",
       "16      gini         21                  6           286  17   0.652526   \n",
       "17      gini         20                  7           250  18   0.657516   \n",
       "18      gini         22                 10           300  19   0.657465   \n",
       "19      gini         10                  6           298  20   0.652996   \n",
       "\n",
       "    elapsed_sec   duration  \n",
       "0     15.584538  12.659195  \n",
       "1     21.130914  18.205591  \n",
       "2     35.085798  13.778017  \n",
       "3     39.129643  23.444741  \n",
       "4     39.753345   4.484855  \n",
       "5     42.905477   2.999732  \n",
       "6     45.822875   2.722389  \n",
       "7     47.139352   7.735802  \n",
       "8     49.793846   3.775522  \n",
       "9     54.876923   4.903988  \n",
       "10    55.052439   7.721802  \n",
       "11    57.610671   2.583051  \n",
       "12    60.414571   2.616628  \n",
       "13    74.481396  13.887527  \n",
       "14    86.281291  31.083730  \n",
       "15   116.864489  42.191943  \n",
       "16   138.583843  52.065018  \n",
       "17   157.324089  40.178739  \n",
       "18   186.280438  47.488718  \n",
       "19   188.729754  31.203226  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d51e46",
   "metadata": {},
   "source": [
    "The `deephyper-analytics` command line is a way of analyzing this type of file. For example, we want to output the best configuration we can use the `topk` functionnality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7da7c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'0':\r\n",
      "  criterion: gini\r\n",
      "  duration: 13.887526989\r\n",
      "  elapsed_sec: 74.4813959599\r\n",
      "  id: 15\r\n",
      "  max_depth: 15\r\n",
      "  min_samples_split: 2\r\n",
      "  n_estimators: 103\r\n",
      "  objective: 0.6638296802\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!deephyper-analytics topk results.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba059c1",
   "metadata": {},
   "source": [
    "Let us define a function `test_config` to evaluate the best configuration on the training, validation and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "081b52ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_config(config):\n",
    "\n",
    "    rs_data = np.random.RandomState(seed=42)\n",
    "\n",
    "    ratio_test = 0.33\n",
    "    ratio_valid = (1 - ratio_test) * 0.33\n",
    "\n",
    "    train, valid, test, _ = dataset.load_data(\n",
    "        random_state=rs_data,\n",
    "        test_size=ratio_test,\n",
    "        valid_size=ratio_valid,\n",
    "        categoricals_to_integers=True,\n",
    "    )\n",
    "\n",
    "    rs_classifier = check_random_state(42)\n",
    "\n",
    "    classifier = RandomForestClassifier(\n",
    "        n_jobs=4,\n",
    "        random_state=rs_classifier,\n",
    "        n_estimators=config[\"n_estimators\"],\n",
    "        criterion=config[\"criterion\"],\n",
    "        max_depth=config[\"max_depth\"],\n",
    "        min_samples_split=config[\"min_samples_split\"],\n",
    "    )\n",
    "    classifier.fit(*train)\n",
    "\n",
    "    acc_train = classifier.score(*train)\n",
    "    acc_valid = classifier.score(*valid)\n",
    "    acc_test = classifier.score(*test)\n",
    "\n",
    "    print(f\"Accuracy on Training: {acc_train:.3f}\")\n",
    "    print(f\"Accuracy on Validation: {acc_valid:.3f}\")\n",
    "    print(f\"Accuracy on Testing: {acc_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e7e0933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training: 0.739\n",
      "Accuracy on Validation: 0.664\n",
      "Accuracy on Testing: 0.663\n"
     ]
    }
   ],
   "source": [
    "best_config = results.iloc[results.objective.argmax()][:-2].to_dict()\n",
    "test_config(best_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0307f865",
   "metadata": {},
   "source": [
    "Compared to the default configuration, we can see the accuracy improvement and the reduction of overfitting between the training and  the validation/test data sets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
